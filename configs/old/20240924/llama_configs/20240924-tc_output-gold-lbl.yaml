source:
  - name: code_contests
    type: json
    kwargs:
      sort_key: id
      path: results/20240924_pseudocode/splitted.json
  - name: example
    type: yaml
    kwargs:
      sort_key: id
      path: configs/20240924/icl_examples-codecontests.yaml

dataset:
  - name: target
    type: dict
    kwargs:
      primary_key: id
      fields:
        - name: id
          source: code_contests
          key: id
        - name: problem
          source: code_contests
          key: problem
        - name: solution
          source: code_contests
          key: solution
        - name: tc_input
          source: code_contests
          key: tc_input
        - name: tc_output-gt
          source: code_contests
          key: tc_output
        - name: pseudocode
          source: code_contests
          key: pc_lbl
          # key: pc_func
          # key: pc_nl
  - name: example
    type: dict
    kwargs:
      primary_key: id
      fields:
        - name: id
          source: example
          key: id
        - name: problem
          source: example
          key: question
        - name: tc_input
          source: example
          key: tc_input
        - name: pseudocode
          source: example
          key: pc_lbl
          # key: pc_func
          # key: pc_nl
        - name: tc_output
          source: example
          key: tc_output-vanilla
          # key: tc_output-lbl-pdb
          # key: tc_output-func-pdb
          # key: tc_output-nl-pdb

# conda activate ArchCode
# export PYTHONPATH="third_party/expand_langchain:$PYTHONPATH"
# python run.py generator --config_path=configs/dbb-testcase.yaml --rerun=False --max_concurrency=16 - run - merge_json - exit
graph:
  entry_point: initialize

  edges:
    - pair: [initialize, __end__]
      type: always

  nodes:
    - name: initialize
      chains:
        - name: tc_output-gen
          dependencies: []
          input_keys: [problem, pseudocode, tc_input]
          type: cot
          kwargs:
            flatten: true
            llm:
              max_tokens: 1000
              num_ctx: 8000
              model: llama3.1:70b-instruct-fp16
              platform: ollama
              temperature: 0
              top_p: 1
              base_url: http://10.1.1.5:11434
            prompt:
              type: chat
              kwargs:
                body_template_paths: ["configs/20240924/templates/tc_output-vanilla"]
                # body_template_paths: ["configs/20240924/templates/tc_output-pdb"]
