source:
  bigcodebench: !inc configs/20240930/configs/inc/source-bigcodebench.yaml
  example: !inc configs/20240930/configs/inc/source-example.yaml

dataset:
  - !inc configs/20240930/configs/inc/dataset-target.yaml
  - name: example
    type: dict
    kwargs:
      primary_key: id
      fields:
        - name: id
          source: example
          key: id
        - name: problem
          source: example
          key: question
        - name: tc_output
          source: example
          key: tc_output
        - name: explanation
          source: example
          key: explanation

# cd ~/Projects/InsertAssertLLM
# conda activate ArchCode
# export PYTHONPATH="third_party/expand_langchain:$PYTHONPATH"
# python run.py generator --config_path=configs/20240924/gpt_configs/20240924-gpt-tc_output-none.yaml --rerun=False --max_concurrency=16 - run - merge_json - exit
graph:
  entry_point: initialize

  edges:
    - pair: [initialize, __end__]
      type: always

  nodes:
    - name: initialize
      chains:
        - name: tc_output-pred-none
          dependencies: []
          input_keys: [problem, tc_input]
          type: cot
          kwargs:
            flatten: true
            llm:
              max_tokens: 8192
              model: gpt-4o-mini
              platform: openai
              temperature: 0
              top_p: 1
            prompt:
              type: chat
              kwargs:
                body_template_paths: ["configs/20240924/templates/tc_output-none"]
